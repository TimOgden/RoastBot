{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bI8mKYZ73VRe"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import collections\n",
    "import pandas\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "#from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NUOs_juZPuDz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Location: localhost_gpu\n",
      "top_k: 16000\n",
      "batch_size: 3\n",
      "num_encoders: 2\n",
      "num_decoders: 2\n",
      "d_model: 256\n",
      "num_heads: 8\n",
      "dff: 1024\n",
      "json: ./reddit_roasts.json\n",
      "img_dir: ./faces/faces/\n",
      "device_type: GPU\n",
      "Distribution Strategy: <tensorflow.python.distribute.one_device_strategy.OneDeviceStrategy object at 0x00000223497EAA08>\n"
     ]
    }
   ],
   "source": [
    "testing = False\n",
    "\n",
    "run_loc = 'localhost_gpu' # options are localhost, colab, and gce\n",
    "\n",
    "run_loc_params = {'top_k': {'localhost': 16000, 'localhost_gpu': 16000, 'colab': 16000, 'gce': 16000},\n",
    "                 'batch_size': {'localhost': 64, 'localhost_gpu': 3, 'colab': 4, 'gce': 4},\n",
    "                 'num_encoders': {'localhost': 12, 'localhost_gpu': 2, 'colab': 12, 'gce': 12},\n",
    "                 'num_decoders': {'localhost': 4, 'localhost_gpu': 2, 'colab': 4, 'gce': 4},\n",
    "                 'd_model': {'localhost': 768, 'localhost_gpu': 256, 'colab': 768, 'gce': 768},\n",
    "                 'num_heads': {'localhost': 12, 'localhost_gpu': 8, 'colab': 8, 'gce': 8},\n",
    "                 'dff': {'localhost': 2048, 'localhost_gpu': 1024, 'colab': 2048, 'gce': 2048},\n",
    "                 'json': {'localhost': './reddit_roasts.json',\n",
    "                          'localhost_gpu': './reddit_roasts.json',\n",
    "                          'colab': './drive/MyDrive/reddit_roasts.json',\n",
    "                          'gce': './reddit_roasts.json'},\n",
    "                 'img_dir': {'localhost': './faces/faces/',\n",
    "                             'localhost_gpu': './faces/faces/',\n",
    "                             'colab': './drive/MyDrive/faces/',\n",
    "                             'gce': './faces/faces/'},\n",
    "                 'device_type': {'localhost': 'CPU', 'localhost_gpu': 'GPU', 'colab': 'GPU', 'gce': 'GPU'}}\n",
    "run_params = {key: run_loc_params[key][run_loc] for key in run_loc_params.keys()}\n",
    "\n",
    "devices = tf.config.list_logical_devices(run_params['device_type'])    \n",
    "if len(devices) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy(devices)\n",
    "else:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(devices[0])\n",
    "print('Run Location: {}'.format(run_loc), '\\n'.join(['{}: {}'.format(key, item) for (key, item) in run_params.items()]), sep='\\n')\n",
    "print('Distribution Strategy:', strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_loc == 'colab':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zhdiHOh9Y-mD"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(1000, (2*(i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UqHVOyw4ZJw4"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Zf_2fgjWqc51"
   },
   "outputs": [],
   "source": [
    "if testing:\n",
    "    pos_encoding = positional_encoding(64, d_model=256)\n",
    "    plt.figure(figsize=(24,20))\n",
    "    plt.imshow(pos_encoding[0])\n",
    "    plt.xlabel('Depth in Encoding Vector')\n",
    "    plt.ylabel('Position in Sequence')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6LcEXGWaT6xw"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = run_params['batch_size']\n",
    "num_patches = 576\n",
    "patch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing:\n",
    "    temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "    y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "    out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "    print(out.shape, attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "S3I_x2h53k80"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, d_model=256, dff=2048, dropout=.1, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.self_attention = tf.keras.layers.MultiHeadAttention(num_heads, key_dim=d_model)\n",
    "        self.fc1 = tf.keras.layers.Dense(dff, activation=tf.keras.activations.gelu)\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attention_weights, _ = self.self_attention(x, x, attention_mask=mask,\n",
    "                                                    return_attention_scores=True)\n",
    "        normalized = self.layer_norm1(x + attention_weights)\n",
    "        x = self.fc2(self.dropout(self.fc1(normalized), training=training))\n",
    "        return self.layer_norm2(x + normalized)\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(dff, activation=tf.keras.activations.gelu)\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "        seq_len = x.shape[1]\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, mask=look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        \n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, mask=padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.fc2(self.dropout3(self.fc1(out2), training=training))\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "jUOa9E12T6xy"
   },
   "outputs": [],
   "source": [
    "class Patches(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"SAME\",\n",
    "        )\n",
    "        \n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "class PatchEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = tf.keras.layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.expand_dims(tf.range(start=0, limit=self.num_patches, delta=1), 0)\n",
    "        projection = self.projection(patch)\n",
    "        embeddings = self.position_embedding(positions)\n",
    "        encoded = projection + embeddings\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7YYnoHRGtk2n"
   },
   "outputs": [],
   "source": [
    "if testing:\n",
    "    img_array = tf.random.uniform([64,384,384,3], minval=0, maxval=1)\n",
    "    print(img_array.shape)\n",
    "    patches = Patches(patch_size)(img_array)\n",
    "    encoded = PatchEncoder(num_patches, 256)(patches)\n",
    "    encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "446bVg6JQBn2"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               maximum_positional_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.patches = Patches(16)\n",
    "        self.patch_encoder = PatchEncoder(num_patches, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_positional_encoding,\n",
    "                                                self.d_model)\n",
    "        self.enc_layers = [EncoderLayer(num_heads, d_model, dff, dropout=rate)\n",
    "                          for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        patches = self.patches(x)\n",
    "        x = self.patch_encoder(patches)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xZeduFo9beWm"
   },
   "outputs": [],
   "source": [
    "if testing:\n",
    "    sample_encoder = Encoder(num_layers=2, d_model=768, num_heads=12,\n",
    "                          dff=2048, maximum_positional_encoding=32)\n",
    "    temp_input = tf.random.uniform((BATCH_SIZE,384,384,3), dtype=tf.float32, minval=0, maxval=1.)\n",
    "    sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "    print(sample_encoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBROLU20wEk-",
    "outputId": "7257743e-c2d7-4b3f-ea86-6da3b476e8dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 1, 10), dtype=float32, numpy=array([[[[0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq,0),tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :] # (batch_size, 1, 1, seq_len)\n",
    "    #return seq[:, :, tf.newaxis]\n",
    "create_padding_mask([[5, 4, 3, 2, 1, 0, 0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dWnWyDZi-L1I",
    "outputId": "48e18e56-a5b6-43bf-a799-ad3dd693d358"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 1., 1.],\n",
       "       [0., 0., 1., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size,size)), -1, 0)\n",
    "    return mask # (seq_len, seq_len)\n",
    "create_look_ahead_mask(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Iue3Un_2b82g"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        #print('Seq length:', seq_len)\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        #print('Sequence shape:', x.shape)\n",
    "        #print('Encoder output shape:', enc_output.shape)\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        #print('Before decoder layers shape:', x.shape)\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zTwmaXkYlbWz"
   },
   "outputs": [],
   "source": [
    "if testing:\n",
    "    sample_decoder = Decoder(num_layers=2, d_model=256, num_heads=8,\n",
    "                          dff=1024, target_vocab_size=8000,\n",
    "                          maximum_position_encoding=5000)\n",
    "    temp_input = tf.random.uniform((BATCH_SIZE, 74), dtype=tf.int64, minval=0, maxval=200)\n",
    "    padding_mask = tf.random.uniform((BATCH_SIZE, 74, 576), dtype=tf.int32, minval=0, maxval=1)\n",
    "    look_ahead_mask = create_look_ahead_mask(74)\n",
    "    output, attn = sample_decoder(temp_input,\n",
    "                                enc_output=sample_encoder_output,\n",
    "                                training=False,\n",
    "                                look_ahead_mask=look_ahead_mask,\n",
    "                                padding_mask=None)\n",
    "\n",
    "    print(output.shape, attn['decoder_layer1_block2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "DH7I7d3vIQc1"
   },
   "outputs": [],
   "source": [
    "if testing:\n",
    "    with tf.device('/cpu:0'):\n",
    "        mha = tf.keras.layers.MultiHeadAttention(8, 256)\n",
    "        enc_output = tf.random.uniform([64,576,256])\n",
    "        dec_latent = tf.random.uniform([64,74,256])\n",
    "        mask = tf.random.uniform([64,74,576], dtype=tf.float32, maxval=1)\n",
    "        print(mha(dec_latent, enc_output, enc_output, mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "5StdCsHjT6x1"
   },
   "outputs": [],
   "source": [
    "def augmentation_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(factor=(-.125,.125)),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomContrast(factor=(.5,1.5))\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "kNfjQsT6l9Hy"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_encoders, num_decoders, d_model, num_heads, dff,\n",
    "               maximum_positional_encoding, target_vocab_size, target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.augmenter = augmentation_model()\n",
    "        self.tokenizer = Encoder(num_encoders, d_model, num_heads, dff,\n",
    "                                maximum_positional_encoding, rate)\n",
    "        self.decoder = Decoder(num_decoders, d_model, num_heads, dff,\n",
    "                               target_vocab_size, target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "  \n",
    "    def call(self, x):\n",
    "        inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask = x\n",
    "        enc_output = self.tokenizer(self.augmenter(inp), training, enc_padding_mask)\n",
    "\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask\n",
    "        )\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "    \n",
    "    def train_step(self, data, *args, **kwargs):\n",
    "        inp, target = data\n",
    "        tar_inp = target[:, :-1]\n",
    "        tar_real = target[:, 1:]\n",
    "        combined_mask, dec_padding_mask = create_masks(tar_inp)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred, _ = self((inp, tar_inp,\n",
    "                                     True,\n",
    "                                     None,\n",
    "                                     combined_mask,\n",
    "                                     None), training=True)\n",
    "            loss = self.compiled_loss(tar_real, y_pred, regularization_losses=self.losses)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(tar_real, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        inp, target = data\n",
    "        tar_inp = target[:, :-1]\n",
    "        tar_real = target[:, 1:]\n",
    "        combined_mask, dec_padding_mask = create_masks(tar_inp)\n",
    "        y_pred, _ = self((inp, tar_inp,\n",
    "                             True,\n",
    "                             None,\n",
    "                             combined_mask,\n",
    "                             None), training=False)\n",
    "        loss = self.compiled_loss(tar_real, y_pred, regularization_losses=self.losses)\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(tar_real, y_pred)\n",
    "        # Return a dict mapping metric names to current value.\n",
    "        # Note that it will include the loss (tracked in self.metrics).\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "v9111tMwT6x3"
   },
   "outputs": [],
   "source": [
    "def create_masks(tar):\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by\n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return combined_mask, dec_target_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "-DJ6QRiQPjm_"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6Q-AXffLT6x3"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=-1), dtype=real.dtype))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "e8VPKD2uvXst"
   },
   "outputs": [],
   "source": [
    "if testing:\n",
    "    sample_transformer = Transformer(\n",
    "      num_encoders=1, num_decoders=1, d_model = 256, num_heads = 8, dff = 2048,\n",
    "      target_vocab_size = 8000, target=6000, maximum_positional_encoding=512)\n",
    "    sample_transformer.compile(loss=loss_function, metrics=[accuracy_function])\n",
    "    x = tf.random.uniform((16, 384, 384, 3))\n",
    "    y = tf.random.uniform((16, 32), minval=0, maxval=4000)\n",
    "    with tf.device('/cpu:0'):\n",
    "        sample_transformer.evaluate(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2KTJ5CRz0OG",
    "outputId": "f7e60679-4342-4d75-fc02-9f11bdbf2421"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=7.987412>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_example = tf.convert_to_tensor([[79,80,50, 0, 0, 0]])\n",
    "logits = tf.one_hot([79,80,50, 3, 3, 3], 8000) * 1\n",
    "pred_example = tf.expand_dims(logits, 0)\n",
    "accuracy_function(real_example, pred_example), loss_function(real_example, pred_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uA44FALewWX",
    "outputId": "edf6d137-575f-41eb-95b6-1992cd8bbf8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0. 1. 0.], shape=(5,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=8.9874115>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.one_hot([3,3,3, 3, 3, 3], 8000) * 1\n",
    "print(logits[0][:5])\n",
    "pred_example = tf.expand_dims(logits, 0)\n",
    "accuracy_function(real_example, pred_example), loss_function(real_example, pred_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "EI7BZK2cJeg0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model:  256 dff:  1024 num_heads:  8\n"
     ]
    }
   ],
   "source": [
    "d_model = run_params['d_model']\n",
    "dff = run_params['dff']\n",
    "num_heads = run_params['num_heads']\n",
    "num_encoders = run_params['num_encoders']\n",
    "num_decoders = run_params['num_decoders']\n",
    "print('d_model: ', d_model, 'dff: ', dff, 'num_heads: ', num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "tmkNWqygT6x3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-0802d4d986b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m transformer = Transformer(num_encoders=num_encoders, num_decoders=num_decoders, d_model=d_model, \n\u001b[0;32m      2\u001b[0m                           \u001b[0mnum_heads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                           \u001b[0mtarget_vocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m                           maximum_positional_encoding=32, rate=.3)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'top_k' is not defined"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(num_encoders=num_encoders, num_decoders=num_decoders, d_model=d_model, \n",
    "                          num_heads = num_heads, dff = dff,\n",
    "                          target_vocab_size = top_k, target=top_k,\n",
    "                          maximum_positional_encoding=32, rate=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nrsvi0P6beoN"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, baseline_lr = 1e-4):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.baseline_lr = baseline_lr\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        val = tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "        return (tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2) * self.baseline_lr) / self.baseline_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "S-u_NLz7H-cJ",
    "outputId": "3dc72319-7edd-478b-aa24-51dc920f0d8d"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model, baseline_lr=1e-4)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.plot(learning_rate(tf.range(0, limit=1e5)))\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Custom Scheduler')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XObPEC9TbfrV"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint('checkpoints/', save_best_only=True, save_weights_only=True, verbose=1),\n",
    "            tf.keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True, verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbRKFkwkT6x4"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.cast(tf.image.resize(img, (384, 384)), tf.float32) / 255.\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yb-IfWqRT6x4"
   },
   "outputs": [],
   "source": [
    "def get_data(path, annotations_json, is_roast=False, split=[.8,.1,.1], subset=None, tokenizer=None, \n",
    "             batch_size=16, return_text=False):\n",
    "    with open(annotations_json, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    image_path_to_caption = collections.defaultdict(list)\n",
    "    if is_roast:\n",
    "        for entry in annotations['info']:\n",
    "            for roast in entry['Roasts']:\n",
    "                if roast != 'removed':\n",
    "                    temp_caption = re.sub('([.,\\'\"!?()0-9])', r' \\1 ', roast)\n",
    "                    caption = f\"<start> {temp_caption} <end>\"\n",
    "                    image_path = path + '{}.jpg'.format(entry['ID'])\n",
    "                    image_path_to_caption[image_path].append(caption)\n",
    "    else:\n",
    "        for val in annotations['annotations']:\n",
    "            temp_caption = re.sub('([.,\\'\"!?()0-9])', r' \\1 ', val['caption'])\n",
    "            caption = f\"<start> {temp_caption} <end>\"\n",
    "            image_path = path + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "            image_path_to_caption[image_path].append(caption)\n",
    "    \n",
    "    image_paths = list(image_path_to_caption.keys())\n",
    "    random.shuffle(image_paths)\n",
    "    \n",
    "    usable_image_paths = image_paths if not subset else image_paths[:subset]\n",
    "    \n",
    "    num = len(usable_image_paths)\n",
    "    train_image_paths = usable_image_paths[:int(num*split[0])]\n",
    "    test_image_paths = usable_image_paths[int(num*split[0]):int(num*split[0]+num*split[1])]\n",
    "    val_image_paths = usable_image_paths[int(num*split[0]+num*split[1]):]\n",
    "    \n",
    "    if not tokenizer:\n",
    "        tokenizer = create_tokenizer_from_texts(top_k, [image_path_to_caption[path] for path in train_image_paths])\n",
    "    \n",
    "    train_dataset = get_dataset(train_image_paths, image_path_to_caption, batch_size, tokenizer)\n",
    "    test_dataset = get_dataset(test_image_paths, image_path_to_caption, batch_size, tokenizer)\n",
    "    val_dataset = get_dataset(val_image_paths, image_path_to_caption, batch_size, tokenizer)\n",
    "    if return_text:\n",
    "        return (train_dataset, test_dataset, val_dataset), tokenizer, [image_path_to_caption[path] for path in train_image_paths]\n",
    "    return (train_dataset, test_dataset, val_dataset), tokenizer\n",
    "\n",
    "def get_dataset(image_paths, image_path_to_caption, batch_size, tokenizer):\n",
    "    captions = []\n",
    "    img_name_vector = []\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        caption_list = image_path_to_caption[image_path]\n",
    "        captions.extend(caption_list)\n",
    "        img_name_vector.extend([image_path] * len(caption_list))\n",
    "        \n",
    "    sequences = tokenizer.texts_to_sequences(captions)\n",
    "    cap_vector = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post', maxlen=32)\n",
    "    #cap_vector = sequences\n",
    "    img_to_cap_vector = collections.defaultdict(list)\n",
    "    for img, cap in zip(img_name_vector, cap_vector):\n",
    "        img_to_cap_vector[img].append(cap)\n",
    "    \n",
    "    BUFFER_SIZE = 1000\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_name_vector, cap_vector))\n",
    "    dataset = dataset.map(lambda x, y: (load_image(x)[0], y))\n",
    "    # Shuffle and batch\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE, seed=40).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def create_tokenizer_from_texts(top_k, texts):\n",
    "    total_texts = []\n",
    "    for text in texts:\n",
    "        for t in text:\n",
    "            total_texts.append(t)\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                     oov_token='<unk>',\n",
    "                                                     filters='#$%&*+.,-/;=@[\\]^_`{|}~')\n",
    "    tokenizer.fit_on_texts(total_texts)\n",
    "    tokenizer.word_index['<pad>'] = 0\n",
    "    tokenizer.index_word[0] = '<pad>'\n",
    "    #tokenizer.word_index['<end>'] = top_k-1\n",
    "    #tokenizer.index_word[top_k-1] = '<end>'\n",
    "    return tokenizer\n",
    "\n",
    "def create_tokenizer_from_file(top_k, filename):\n",
    "    total_texts = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            total_texts.append(line)\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                     oov_token='<unk>',\n",
    "                                                     filters='#$%&*+.,-/;=@[\\]^_`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts(total_texts)\n",
    "    tokenizer.word_index['<pad>'] = 0\n",
    "    tokenizer.index_word[0] = '<pad>'\n",
    "    #tokenizer.word_index['<end>'] = top_k-1\n",
    "    #tokenizer.index_word[top_k-1] = '<end>'\n",
    "    return tokenizer\n",
    "\n",
    "def save_all_train_annotations(filename, texts):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for text in texts:\n",
    "            f.writelines([t+'\\n' for t in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4Tf5BoWT6x4",
    "outputId": "346a8736-6871-45e0-ce35-e0d25d38cd96"
   },
   "outputs": [],
   "source": [
    "re.sub('([.,\\'\"!?()0-9])', r' \\1 ', 'Testing! All aboard? We (not they) are \"testing\" [now]. Let\\'s go, buddy. 123456 I\\'m not happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgXQBFjVT6x5"
   },
   "outputs": [],
   "source": [
    "roast_annotation_file = run_params['json']\n",
    "REDDIT_PATH = run_params['img_dir']\n",
    "\n",
    "print('Roasts file: ', roast_annotation_file)\n",
    "print('Image directory: ', REDDIT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if run_loc == 'colab':\n",
    "    with open('./drive/MyDrive/master_tokenizer.pickle', 'rb') as f:\n",
    "        roast_tokenizer = pickle.load(f)\n",
    "else:\n",
    "    with open('master_tokenizer.pickle', 'rb') as f:\n",
    "        roast_tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZ9GRVnET6x6"
   },
   "outputs": [],
   "source": [
    "def show_example(dataset, tokenizer):\n",
    "    for (imgs, captions) in dataset.take(1):\n",
    "        img = imgs[0]\n",
    "        plt.imshow(imgs[0])\n",
    "        plt.show()\n",
    "        print('Sequence:', captions[0])\n",
    "        print('Caption:', [tokenizer.index_word[i] for i in captions[0].numpy() if i != 0])\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxkENacNT6x7"
   },
   "outputs": [],
   "source": [
    "(roast_train_dataset, roast_test_dataset, roast_val_dataset), roast_tokenizer = get_data(REDDIT_PATH, roast_annotation_file, is_roast=True,\n",
    "                                                                     subset=None, batch_size=BATCH_SIZE,\n",
    "                                                                           tokenizer=roast_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVzVPoZehqNL",
    "outputId": "8bc612ea-87fb-498e-f243-1634b14b9c17"
   },
   "outputs": [],
   "source": [
    "roast_train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "JpKZZzoHT6x7",
    "outputId": "8b2c9fff-1115-46b3-a296-b2ef06088d97"
   },
   "outputs": [],
   "source": [
    "show_example(roast_train_dataset, roast_tokenizer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxHoRFO0T6x8",
    "outputId": "8ea0d36a-7320-4c50-b4f0-58d902048ae8"
   },
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SY0VweMQLOAh",
    "outputId": "5d06cad5-d9f2-43fd-f7a2-36f0ef99d503"
   },
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "print('\\n'.join(gpu_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f234b47b00974efc92e9b90caf5a905c",
      "f36b519f7a4c43559ee70366df325663",
      "b3addd83bd3e465084d666cf422d87d6",
      "c3c105cae7824fc08417b8baf12bd0d7",
      "a0d57f89eda443dbb7484a0ce564eaf4",
      "0fdd6e2694774d9986a711ea63794d5f",
      "a3ad3aa849f84d2b86dda5e059b2cb03",
      "c7add4a9da914da28781c892a23e870c"
     ]
    },
    "id": "P2DD7Ra6T6x9",
    "outputId": "db017bf7-ad7d-41f0-a2c1-d89ec01d4ca3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device(run_params['device']):\n",
    "    transformer.fit(roast_train_dataset, validation_data=roast_test_dataset, epochs=9, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XS6R_J_T6x-"
   },
   "outputs": [],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.evaluate(roast_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWaxmZTUgxij"
   },
   "outputs": [],
   "source": [
    "def greedy_decoder(predictions):\n",
    "  #print(tf.argmax(predictions, axis=-1), tf.argmin(predictions, axis=-1))\n",
    "  return tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "def greedy_non_unk_decoder(predictions):\n",
    "    vals, indices = tf.math.top_k(predictions, k=2)\n",
    "    return tf.cast(tf.reduce_sum(vals * tf.cast(tf.math.logical_not(tf.math.equal(indices, 4)), dtype=tf.float32)), tf.int32)\n",
    "\n",
    "def beam_search_decoder(predictions, beam_width, top_paths):\n",
    "    sequence_lengths = tf.cast(tf.fill([predictions.shape[0]], predictions.shape[1]), tf.int32)\n",
    "    predictions = tf.transpose(predictions, perm=[1,0,2])\n",
    "    print(sequence_lengths)\n",
    "    print(predictions.shape)\n",
    "\n",
    "    decoded, log_probabilities = tf.compat.v1.nn.ctc_beam_search_decoder(\n",
    "                  predictions, sequence_lengths, beam_width=beam_width, top_paths=top_paths, merge_repeated=False\n",
    "            )\n",
    "    print('Decoded:', tf.sparse.to_dense(decoded[0]))\n",
    "    print('Log probabilities:', log_probabilities)\n",
    "    return tf.sparse.to_dense(decoded[0])\n",
    "\n",
    "def probabilistic_decoder(predictions, temperature):\n",
    "    predicted_id = tf.random.categorical(predictions[0]/temperature, num_samples=1)[0]\n",
    "    return tf.expand_dims(tf.cast(predicted_id, tf.int32),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAyiE-WFT6x_"
   },
   "outputs": [],
   "source": [
    "def generate_roast(image, tokenizer, start_text=None, max_length=75, temperature=1.):\n",
    "    encoder_input = tf.expand_dims(image,0)\n",
    "    start, end = tokenizer.word_index['<start>'], tokenizer.word_index['<end>']\n",
    "    if start_text:\n",
    "        output = [start] + tokenizer.texts_to_sequences([start_text])[0]\n",
    "    else:\n",
    "        output = [start]\n",
    "    output = tf.convert_to_tensor(output)\n",
    "    output = tf.expand_dims(output, 0)\n",
    "    for i in range(max_length-len(output)):\n",
    "        lookahead_mask, dec_padding_mask = create_masks(output)\n",
    "        # predictions.shape = (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer((encoder_input,\n",
    "                                                    output,\n",
    "                                                    False,\n",
    "                                                    None,\n",
    "                                                    lookahead_mask,\n",
    "                                                    None))\n",
    "        #print(predictions.shape)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        #print(tf.argmax(predictions, axis=-1))\n",
    "        predicted_id = greedy_decoder(predictions)\n",
    "        #predicted_id = probabilistic_decoder(predictions, temperature=temperature)\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        if predicted_id == end:\n",
    "            print('Found end')\n",
    "            break\n",
    "    print(output.shape)\n",
    "    text = tokenizer.sequences_to_texts(output.numpy())[0]\n",
    "    return text, attention_weights\n",
    "\n",
    "def plot_predictions(image, tokenizer, start_text=None, max_length=75, temperature=1., subset=25):\n",
    "    encoder_input = tf.expand_dims(image,0)\n",
    "    start, end = tokenizer.word_index['<start>'], tokenizer.word_index['<end>']\n",
    "    if start_text:\n",
    "        output = [start] + tokenizer.texts_to_sequences([start_text])[0]\n",
    "    else:\n",
    "        output = [start]\n",
    "    output = tf.convert_to_tensor(tf.expand_dims(output,0))\n",
    "    \n",
    "    lookahead_mask, dec_padding_mask = create_masks(output)\n",
    "    \n",
    "    print('Encoder input:', encoder_input)\n",
    "    print('Decoder input:', output)\n",
    "    print('Lookahead mask:', lookahead_mask)\n",
    "    # predictions.shape = (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer((encoder_input,\n",
    "                                                output,\n",
    "                                                False,\n",
    "                                                None,\n",
    "                                                lookahead_mask,\n",
    "                                                None))\n",
    "    print('Predictions:', tf.argmax(predictions, axis=-1))\n",
    "    predictions = predictions[:, -1:, :subset]\n",
    "    print(predictions)\n",
    "    x_ticks = np.arange(8000)[:subset]\n",
    "\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    ax = plt.gca()\n",
    "    ax.bar(x_ticks, predictions[0][0])\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels([tokenizer.index_word[i] for i in x_ticks], rotation=90)\n",
    "    plt.show()\n",
    "    predicted_id = greedy_decoder(predictions)\n",
    "    print('Argmax:', predicted_id)\n",
    "    print('Prediction:', tokenizer.index_word[predicted_id.numpy()[0][0]])\n",
    "    \n",
    "def print_input_pred(image, tokenizer, start_text=None):\n",
    "    encoder_input = tf.expand_dims(image,0)\n",
    "    start, end = tokenizer.word_index['<start>'], tokenizer.word_index['<end>']\n",
    "    if start_text:\n",
    "        output = [start] + tokenizer.texts_to_sequences([start_text])[0]\n",
    "    else:\n",
    "        output = [start]\n",
    "    output = tf.convert_to_tensor(tf.expand_dims(output,0))\n",
    "    \n",
    "    lookahead_mask, dec_padding_mask = create_masks(output)\n",
    "    \n",
    "    predictions, attention_weights = transformer((encoder_input,\n",
    "                                                output,\n",
    "                                                False,\n",
    "                                                None,\n",
    "                                                lookahead_mask,\n",
    "                                                None))\n",
    "    print(lookahead_mask)\n",
    "    print(output.shape, predictions.shape)\n",
    "    print('Input:',' '.join([tokenizer.index_word[i] for i in output[0].numpy()]))\n",
    "    print('Predictions:',' '.join([tokenizer.index_word[i] for i in tf.argmax(predictions, axis=-1)[0].numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTwHprvkT6x_"
   },
   "outputs": [],
   "source": [
    "img = show_example(roast_val_dataset, roast_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zMegcaLT6x_",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_size = 384\n",
    "patch_size = 16\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([img]), size=(image_size, image_size)\n",
    ")\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy())\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYhOZIiI6tzu"
   },
   "outputs": [],
   "source": [
    "roast_tokenizer.index_word[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(print_freq=200):\n",
    "    start = time.time()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "    for (batch, (inp, tar)) in tqdm(enumerate(roast_test_dataset)):\n",
    "        test_step(inp, tar)\n",
    "        \n",
    "        if batch % print_freq == 0:\n",
    "            print(f'Test step: Epoch {current_epoch + 1} Batch {batch} Loss {test_loss.result():.4f} Accuracy {test_accuracy.result():.4f}')\n",
    "    print('Eval loss:', test_loss)\n",
    "    print('Eval accuracy:', test_accuracy)\n",
    "#eval_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01kxlLA0jg5_"
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    plot_predictions(img, roast_tokenizer, start_text='Only', subset=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7Hlw8LpT6yA"
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    text, attention_weights = generate_roast(img, roast_tokenizer, temperature=1., start_text=None)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    print_input_pred(img, roast_tokenizer, start_text=\"this is a test to see if the neural network repeats itself, I believe that it will but we shall see\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFNZMmxL1P1c"
   },
   "outputs": [],
   "source": [
    "roast_tokenizer.index_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WK32iDyrZCP"
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RoastTransformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0fdd6e2694774d9986a711ea63794d5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0d57f89eda443dbb7484a0ce564eaf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a3ad3aa849f84d2b86dda5e059b2cb03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b3addd83bd3e465084d666cf422d87d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fdd6e2694774d9986a711ea63794d5f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0d57f89eda443dbb7484a0ce564eaf4",
      "value": 1
     }
    },
    "c3c105cae7824fc08417b8baf12bd0d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7add4a9da914da28781c892a23e870c",
      "placeholder": "",
      "style": "IPY_MODEL_a3ad3aa849f84d2b86dda5e059b2cb03",
      "value": " 8053/? [3:25:29&lt;00:00,  1.52s/it]"
     }
    },
    "c7add4a9da914da28781c892a23e870c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f234b47b00974efc92e9b90caf5a905c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b3addd83bd3e465084d666cf422d87d6",
       "IPY_MODEL_c3c105cae7824fc08417b8baf12bd0d7"
      ],
      "layout": "IPY_MODEL_f36b519f7a4c43559ee70366df325663"
     }
    },
    "f36b519f7a4c43559ee70366df325663": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
